{"cells":[{"cell_type":"markdown","source":["#### Data Description\n\n- This tutorial makes use of the California Housing data set. It appeared in a 1997 paper titled Sparse Spatial Autoregressions, written by Pace, R. Kelley and Ronald Barry and published in the Statistics and Probability Letters journal. The researchers built this data set by using the 1990 California census data.\n\n- The data contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). In this sample a block group on average includes 1425.5 individuals living in a geographically compact area. \n\nThese spatial data contain 20,640 observations on housing prices with 9 economic variables:\n\n* **Longitude** refers to the angular distance of a geographic place north or south of the earth’s equator for each block group;\n* **Latitude** refers to the angular distance of a geographic place east or west of the earth’s equator for each block group;\n* **Housing median age** is the median age of the people that belong to a block group. \n* **Total rooms** is the total number of rooms in the houses per block group;\n* **Total bedrooms** is the total number of bedrooms in the houses per block group;\n* **Population** is the number of inhabitants of a block group;\n* **Households** refers to units of houses and their occupants per block group;\n* **Median income** is used to register the median income of people that belong to a block group; \n* **Median house value** is the dependent variable and refers to the median house value per block group.\n\n**The Median house value** is the dependent variable and will be assigned the role of the **target variable** in your ML model."],"metadata":{}},{"cell_type":"markdown","source":["#### Data Loading"],"metadata":{}},{"cell_type":"code","source":["# import necessary libs\nimport numpy  as np\nimport pandas as pd\n\n# general spark modules\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType #https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\n\n# spark ml modules \nfrom pyspark.ml.linalg import DenseVector\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorIndexer"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# just execute it once\nACCESS_KEY = \"\"\nSECRET_KEY = \"\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"\"\nMOUNT_NAME = \"\"\n\n# only execute this line once\ntry: \n  dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\nexcept:\n  pass"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Load in the data\nrdd = sc.textFile(\"/mnt/%s/cal_housing.data\" % MOUNT_NAME)\n\n# Load in the header\nheader = sc.textFile(\"/mnt/%s/cal_housing.domain\" % MOUNT_NAME)\n\n# Split lines on commas\nrdd = rdd.map(lambda line: line.split(\",\"))\n\n# Inspect the first 2 lines \nrdd.first()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Map the RDD to a DF\ndf = rdd.map(lambda line: Row(longitude=line[0], \n                              latitude=line[1], \n                              housingMedianAge=line[2],\n                              totalRooms=line[3],\n                              totalBedRooms=line[4],\n                              population=line[5], \n                              households=line[6],\n                              medianIncome=line[7],\n                              medianHouseValue=line[8])).toDF()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Show the top 20 rows \ndf.show(5)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Write a custom function to convert the data type of DataFrame columns\ndef convertColumn(df, names, newType):\n  for name in names: \n     df = df.withColumn(name, df[name].cast(newType))\n  return df \n\n# Assign all column names to `columns`\ncolumns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n\n# Conver the `df` columns to `FloatType()`\ndf = convertColumn(df, columns, FloatType())"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# lets calculate some basic statistics about data\ndf.select('households', 'housingMedianAge', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms').describe().show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Data Preprocessing"],"metadata":{}},{"cell_type":"code","source":["# conversion of target variable to improve stability of algorithms\n@pandas_udf('double', PandasUDFType.SCALAR)\ndef log1p(v):\n      return np.log1p(v)\n\ndf = df.withColumn('medianHouseValueLog', log1p(df.medianHouseValue))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Add the new columns to `df`\ndf = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Re-order and select columns\ndf_for_ml = df.select(\"medianHouseValueLog\", \n                      \"totalBedRooms\", \n                      \"totalRooms\",\n                      \"population\", \n                      \"households\", \n                      \"medianIncome\", \n                      \"roomsPerHousehold\", \n                      \"populationPerHousehold\", \n                      \"bedroomsPerRoom\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Define the `input_data` \ninput_data = df_for_ml.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n\n# Replace `df` with the new DataFrame\ndf_for_ml = spark.createDataFrame(input_data, [\"label\", \"features\"])\n\n# Initialize the `standardScaler`\nstandardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n\n# Fit the DataFrame to the scaler\nscaler = standardScaler.fit(df_for_ml)\n\n# Transform the data in `df` with the scaler\nscaled_df = scaler.transform(df_for_ml)\n\n# Inspect the result\nscaled_df = scaled_df.select('label', col('features_scaled').alias(\"features\"))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["scaled_df.rdd.take(10)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["#### Building A Machine Learning Models With Spark ML\n\n* Linear Regression : https://en.wikipedia.org/wiki/Linear_regression, https://www.youtube.com/watch?v=zPG4NjIkCjc\n* Decision trees    : http://www.r2d3.us/visual-intro-to-machine-learning-part-1/  (nice visualization for decision tree)\n* Random forest     : https://en.wikipedia.org/wiki/Random_forest\n* GB Decision Trees : https://en.wikipedia.org/wiki/Gradient_boosting\n* Clustering        : https://spark.apache.org/docs/2.3.0/ml-clustering.html, https://www.datascience.com/blog/k-means-clustering\n* Cross-validation  : https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n* Collaborative filtering: https://spark.apache.org/docs/2.3.0/mllib-collaborative-filtering.html, https://bugra.github.io/work/notes/2014-04-19/alternating-least-squares-method-for-collaborative-filtering/\n* Metrics:\n  * RMSE: https://en.wikipedia.org/wiki/Root-mean-square_deviation\n  * R2:   https://en.wikipedia.org/wiki/Coefficient_of_determination"],"metadata":{}},{"cell_type":"code","source":["# Split the data into train and test sets\ntrain_data, test_data = scaled_df.randomSplit([0.8,  0.2],  seed=1234)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["##### Linear Regression"],"metadata":{}},{"cell_type":"code","source":["# Initializing of Linear Regression\nlr = LinearRegression(labelCol=\"label\", maxIter=10000, regParam=0.2, elasticNetParam=0.5)\n\n# Fit the data to the model\nlinearModel = lr.fit(train_data)\n\n# Summarize the model over the training set and print out some metrics\ntrainingSummary = linearModel.summary\nprint(\"RMSE train: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"r2   train: %f\" % trainingSummary.r2)\n\n# Generate predictions\npredicted = linearModel.transform(test_data)\n\n# Select (prediction, true label) and compute test error\nevaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\nevaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n\nrmse = evaluator_rmse.evaluate(predicted)\nr2   = evaluator_r2.evaluate(predicted)\n\nprint(\"\\nRMSE test: %f\" % rmse)\nprint(\"r2   test: %f\" % r2)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["##### Linear Regression with Cross-Validation"],"metadata":{}},{"cell_type":"code","source":["# Initializinf of Linear Regression\nlr = LinearRegression(labelCol=\"label\")\n\n# let's set desired parameters\nparamGrid = ParamGridBuilder()\\\n.addGrid(lr.regParam,        [0.2, 0.3, 0.5,  0.7])\\\n.addGrid(lr.elasticNetParam, [0.2, 0.5,  0.7, 0.8])\\\n.addGrid(lr.maxIter,         [100, 1000, 5000, 10000])\\\n.build()\n\n# cross-validation settings\ncrossval = CrossValidator(estimator=lr,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=RegressionEvaluator(),\n                          numFolds=3,\n                          seed=2018\n                         )  \n\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(train_data)\n\n# Summarize the model over the training set and print out some metrics\ntrainingSummary = cvModel.bestModel.summary\nprint(\"RMSE train: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"r2   train: %f\" % trainingSummary.r2)\n\n# Generate predictions\npredicted = cvModel.transform(test_data)\n\n# Select (prediction, true label) and compute test error\nevaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\nevaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n\nrmse = evaluator_rmse.evaluate(predicted)\nr2   = evaluator_r2.evaluate(predicted)\n\nprint(\"\\nRMSE test: %f\" % rmse)\nprint(\"r2   test: %f\" % r2)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#### Decision-Tree with Cross-Validation"],"metadata":{}},{"cell_type":"code","source":["# Automatically identify categorical features, and index them.\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(scaled_df)\n\n# Train a DecisionTree model.\ndt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n\n# Chain indexer and tree in a Pipeline\npipeline = Pipeline(stages=[featureIndexer, dt])\n\nparamGrid = ParamGridBuilder()\\\n    .addGrid(dt.maxDepth, [3,  10, 25])\\\n    .addGrid(dt.maxBins,  [10, 20, 32])\\\n    .build()\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=RegressionEvaluator(),\n                          numFolds=3)  \n\n# Run cross-validation, and choose the best set of parameters.\ncvModel = crossval.fit(train_data)\n\n# Generate predictions\npredicted = cvModel.transform(test_data)\n\n# Select (prediction, true label) and compute test error\nevaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\nevaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n\nrmse = evaluator_rmse.evaluate(predicted)\nr2   = evaluator_r2.evaluate(predicted)\n\nprint(\"\\nRMSE test: %f\" % rmse)\nprint(\"r2   test: %f\" % r2)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["tree = cvModel.bestModel.stages[1]"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["tree.featureImportances"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["#### Random forest"],"metadata":{}},{"cell_type":"code","source":["featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(scaled_df)\n\n# Train a RandomForest model.\nrf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n\n# Chain indexer and forest in a Pipeline\npipeline = Pipeline(stages=[featureIndexer, rf])\n\n# Train model.  This also runs the indexer.\nmodel = pipeline.fit(train_data)\n\n# Generate predictions\npredicted = model.transform(test_data)\n\n# Select (prediction, true label) and compute test error\nevaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\nevaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n\nrmse = evaluator_rmse.evaluate(predicted)\nr2   = evaluator_r2.evaluate(predicted)\n\nprint(\"\\nRMSE test: %f\" % rmse)\nprint(\"r2   test: %f\" % r2)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# additonal outcome from trees is feature importances (can be used for feature selection)\nrf_model = model.stages[1]\nprint(rf_model.featureImportances)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["#### Gradient-boosted tree regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Automatically identify categorical features, and index them.\n# Set maxCategories so features with > 4 distinct values are treated as continuous.\nfeatureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(scaled_df)\n\n# Train a GBT model.\ngbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n\n# Chain indexer and GBT in a Pipeline\npipeline = Pipeline(stages=[featureIndexer, gbt])\n\n# Train model.  This also runs the indexer.\nmodel = pipeline.fit(train_data)\n\n# Generate predictions\npredicted = model.transform(test_data)\n\n# Select (prediction, true label) and compute test error\nevaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\nevaluator_r2   = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n\nrmse = evaluator_rmse.evaluate(predicted)\nr2   = evaluator_r2.evaluate(predicted)\n\nprint(\"\\nRMSE test: %f\" % rmse)\nprint(\"r2   test: %f\" % r2)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["#### Clustering"],"metadata":{}},{"cell_type":"code","source":["# https://rsandstroem.github.io/sparkkmeans.html\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\n# Trains a k-means model.\nkmeans = KMeans().setK(3).setSeed(1)\nmodel = kmeans.fit(scaled_df)\n\n# Make predictions\npredictions = model.transform(scaled_df)\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["display(model, scaled_df)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["display(model, scaled_df)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["#### Collaborative filltering"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\n\n# Load in the header\nlines = spark.read.text(\"/mnt/%s/sample_movielens_ratings.txt\" % MOUNT_NAME).rdd\nparts = lines.map(lambda row: row.value.split(\"::\"))\n\nratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), \n                                     movieId=int(p[1]),\n                                     rating=float(p[2]),\n                                     timestamp=long(p[3])))\n\nratings = spark.createDataFrame(ratingsRDD)\n(training, test) = ratings.randomSplit([0.8, 0.2])\n\n# Build the recommendation model using ALS on the training data\n# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\nals = ALS(maxIter=10,\n          regParam=0.01,\n          userCol=\"userId\",\n          itemCol=\"movieId\",\n          ratingCol=\"rating\",\n          coldStartStrategy=\"drop\")\n\nmodel = als.fit(training)\n\n# Evaluate the model by computing the RMSE on the test data\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))\n\n# Generate top 10 movie recommendations for each user\nuserRecs = model.recommendForAllUsers(10)\n# Generate top 10 user recommendations for each movie\nmovieRecs = model.recommendForAllItems(10)\n\n# Generate top 10 movie recommendations for a specified set of users\nusers = ratings.select(als.getUserCol()).distinct().limit(3)\nuserSubsetRecs = model.recommendForUserSubset(users, 10)\n\n# Generate top 10 user recommendations for a specified set of movies\nmovies = ratings.select(als.getItemCol()).distinct().limit(3)\nmovieSubSetRecs = model.recommendForItemSubset(movies, 10)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# let's take a look for some user recomendations\nuserSubsetRecs.show()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":41}],"metadata":{"name":"Machine Learning in Spark","notebookId":1600962361183146},"nbformat":4,"nbformat_minor":0}
