{"cells":[{"cell_type":"markdown","source":["## Spark Streaming\nStructured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write Ahead Logs. In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n\nInternally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.\n\nIn this guide, we are going to walk you through the programming model and the APIs. We are going to explain the concepts mostly using the default micro-batch processing model, and then later discuss Continuous Processing model. First, letâ€™s start with a simple example of a Structured Streaming query - a streaming word count."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import from_json, to_timestamp\nfrom pyspark.sql.functions import window\nfrom pyspark.sql.functions import avg"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Reading From Kinesis Stream"],"metadata":{}},{"cell_type":"markdown","source":["We will start our streaming application buy reading the data from Kinesis. Kinesis is a message queue, similar to Kafka, provided by AWS. You can read from Kinesis stream in the following way:"],"metadata":{}},{"cell_type":"code","source":["kinesisDF = spark \\\n  .readStream \\\n  .format(\"kinesis\") \\\n  .option(\"streamName\", \"bitcoin-info\") \\\n  .option(\"initialPosition\", \"earliest\") \\\n  .option(\"region\", \"us-west-2\") \\\n  .option(\"awsAccessKey\", '') \\\n  .option(\"awsSecretKey\", '') \\\n  .load()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Its a good idea to clear up old files, streaming application produces a lot of temp files, and there is a limit to how many temp files you can have for a free Databricks account."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm('dbfs:/SOME_CHECKPOINT_DIRECTORY/', True)\ndbutils.fs.rm(('dbfs:/tmp/'), True)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["We will enforce a schema as it is more efficient, if we leave it blank Spark can figure out the Schema as well!"],"metadata":{}},{"cell_type":"code","source":["pythonSchema = StructType().add(\"timestamp\", StringType()).add(\"euro_rate\", FloatType()).add(\"usd_rate\", FloatType()).add (\"gbp_rate\", FloatType())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Now we will read from the stream into our streaming dataframe!"],"metadata":{}},{"cell_type":"code","source":["bitcoinDF = kinesisDF.selectExpr(\"cast (data as STRING) jsonData\").select(from_json(\"jsonData\", pythonSchema).alias(\"bitcoin\")).select(\"bitcoin.*\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["We will also convert the timestamp column to the timestamp type so we can query with datetime object in Python"],"metadata":{}},{"cell_type":"code","source":["bitcoinDF = bitcoinDF.withColumn('timestamp', to_timestamp(bitcoinDF.timestamp, \"yyyy-MM-dd'T'HH:mm:ss\"))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Quering! \nNow you can use all the things you learnt previously from Spark SQL! For example, you can groupBy certain attribute and aggregate, filter, or select as you wish! <br/>\nWe haven't been introduced to the concept of windowing, which we will briefly zoom in now."],"metadata":{}},{"cell_type":"markdown","source":["A window function can also be applied to to Bucketize rows into one or more time windows given a timestamp specifying column. For that we will use window groupBy function (pyspark.sql.functions.window) <br/>\nyou can call the window groupby function in the following way: __window(timeColumn, windowDuration, slideDuration=None, startTime=None)__. The definition of slide interval and window interval are as follows:\n* Window Duration: how far back in time the windowed transformation goes\n* Slide Duration: how often a windowed intransformation is computed"],"metadata":{}},{"cell_type":"code","source":["windowedCounts = bitcoinDF.groupBy(window(bitcoinDF.timestamp, \"10 minutes\", \"5 minutes\").alias('time_window')).agg(avg(bitcoinDF.euro_rate).alias('window_avg_euro_rate'))\ndisplay(windowedCounts)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# to read the stream from memory we will set up a table in our memory called bitcoin_window\nquery = windowedCounts.writeStream.format(\"memory\").queryName(\"bitcoin_window\").outputMode(\"complete\").start()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# once the stream is ready you can query in SQL using the following way\n%sql select time_window.start, time_window.end, window_avg_euro_rate from bitcoin_window limit 5"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# you can also take your stream to a dataframe using SQL queries in the following way\ndf = spark.sql('select time_window.start, time_window.end, window_avg_euro_rate from bitcoin_window')"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# we can query on top of live data for average euro rate for the last hour\nfrom datetime import datetime, timedelta\nfrom pyspark.sql.functions import avg\ntimedelta_ten_mins = datetime.now() - timedelta(minutes=60)\nlast_hour_rate_query = df.filter(df.start > timedelta_ten_mins).select('window_avg_euro_rate').agg(avg('window_avg_euro_rate').alias('rate')).collect()\nprint(last_hour_rate_query[0].rate)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Bitcoin Trading\nWe created a Bitcoin Trading simulator so you can play around with Spark Streaming! There is a virtual wallet per group which can be viewed at: \"your amazon instance\"\nYou can buy and sell bitcoin by using the API. To use the api first configure the user_id and access_key for your group."],"metadata":{}},{"cell_type":"code","source":["user_id = '' # use user_id of your group\naccess_key = '' #use access_key of your group\napi = ''"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["The following API's are available for you to use.\n* __Info:__ can be used to get information about your virtual wallet. You can use this api by sending the following request: __api+'info/?user_id='+user_id__\n* __Sell:__ can be used to sell bitcoin. Just define the amount you want to sell and call the api like: __api+'sell/?amount='+amount+'&user_id=+'+user_id+'&access_key='+access_key__\n*  __Buy:__ can be used to buy bitcoin. Just define the amount you want to sell and call the api like: __api+'buy/?amount='+amount+'&user_id=+'+user_id+'&access_key='+access_key__"],"metadata":{}},{"cell_type":"markdown","source":["## Examples\n\nWe will use the info api to get our current bitcoin for test user in the following way"],"metadata":{}},{"cell_type":"code","source":["import json, requests\n# do a HTTP rest request\nrsp = requests.get( api+'info/?user_id='+user_id).text\n# parse JSON data\nbitcoin = json.loads(rsp)[0]['bitcoin']\neuro = json.loads(rsp)[0]['euro']\n# print\nprint(bitcoin, euro)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Buy \nLets buy a bitcoin!"],"metadata":{}},{"cell_type":"code","source":["# set amount to 1\namount = 1\namount = str(amount)\nrsp = requests.get(api+'buy/?amount='+amount+'&user_id='+user_id+'&access_key='+access_key)\nprint (rsp.text,rsp.status_code)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["rsp = requests.get( api+'info/?user_id='+user_id).text\nbitcoin = json.loads(rsp)[0]['bitcoin']\neuro = json.loads(rsp)[0]['euro']\nprint(bitcoin, euro)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## Sell \nLets sell a bitcoin!"],"metadata":{}},{"cell_type":"code","source":["# set amount to 1\namount = 1\namount = str(amount)\nrsp = requests.get(api+'sell/?amount='+amount+'&user_id='+user_id+'&access_key='+access_key)\nprint (rsp.text,rsp.status_code)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["rsp = requests.get( api+'info/?user_id='+user_id).text\nbitcoin = json.loads(rsp)[0]['bitcoin']\neuro = json.loads(rsp)[0]['euro']\nprint(bitcoin, euro)"],"metadata":{},"outputs":[],"execution_count":31}],"metadata":{"name":"Spark Lab 7","notebookId":1720750739866713},"nbformat":4,"nbformat_minor":0}
