{"cells":[{"cell_type":"code","source":["# Put the correct credentials here and mount S3 bucket\nACCESS_KEY = \"ACCESS_KEY\"\nSECRET_KEY = \"SECRET_KEY\"\nAWS_BUCKET_NAME = \"AWS_BUCKET_NAME\"\nMOUNT_NAME = \"MOUNT_NAME\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n\ntry: \n  dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\nexcept:\n  pass"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# For now we will just read data from a text file\ninput_rdd = sc.textFile(\"/mnt/%s/flights.csv\" % MOUNT_NAME)\nflight_delay_list_rdd = input_rdd.filter(lambda line: 'YEAR' not in line).map(lambda line: line.split(','))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Transformations on RDD of key-value Pairs"],"metadata":{}},{"cell_type":"code","source":["print((input_rdd.map(lambda line: line.split(',')).first()[4]),(input_rdd.map(lambda line: line.split(',')).first()[11]))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# lets do some data cleaning, lets just assume the delay to be 0 if the value is missing\n# since it is a small portion of our dataset it doesn't matter\n# however the right approach is to take the mean to handle scenarios of missing data\nairline_departureDelay_rdd = flight_delay_list_rdd.map(lambda line: (line[4], int(line[11]) if line[11] != '' else 0))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# we just created a key-value pair RDD with airline and arrival_delay\nairline_departureDelay_rdd.take(10)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# lets do some cleaning, remove negative departure delay and consider them as 0\nairline_posDepartureDelay_rdd = airline_departureDelay_rdd.map(lambda line: (line[0], line[1] if line[1] > 0 else 0))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["airline_posDepartureDelay_rdd.take(10)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### reduceByKey\n\nThe higher-order reduceByKey method takes an associative binary operator as input and reduces values with the same key to a single value using the specified binary operator.\n\nA binary operator takes two values as input and returns a single value as output. An associative operator returns the same result regardless of the grouping of the operands.\n\nThe reduceByKey method can be used for aggregating values by key. For example, it can be used for calculating sum, product, minimum or maximum of all the values mapped to the same key."],"metadata":{}},{"cell_type":"code","source":["# reduce by key (airline) to get the total departure delay per airline\nposDepartureDelay_reduced_rdd = airline_posDepartureDelay_rdd.reduceByKey(lambda value1, value2: value1 + value2)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# airline late departure sorted print\nsorted(posDepartureDelay_reduced_rdd.collect(), key=lambda x: x[1])"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### groupByKey\n\nThe groupByKey method returns an RDD of pairs, where the first element in a pair is a key from the source RDD and the second element is a collection of all the values that have the same key. It is similar to the groupBy method that we saw earlier. The difference is that groupBy is a higher-order method that takes as input a function that returns a key for each element in the source RDD. The groupByKey method operates on an RDD of key-value pairs, so a key generator function is not required as input.\n\n** The groupByKey method should be avoided. It is an expensive operation since it may shuffle data. For most use cases, better alternatives are available. ** \n\nhttps://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html"],"metadata":{}},{"cell_type":"markdown","source":["### join\nThe join method takes an RDD of key-value pairs as input and performs an inner join on the source and input RDDs. It returns an RDD of pairs, where the first element in a pair is a key found in both source and input RDD and the second element is a tuple containing values mapped to that key in the source and input RDD."],"metadata":{}},{"cell_type":"code","source":["# We want to get the Airlines names, not just the code\nposDepartureDelay_reduced_rdd.first()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# lucky for us, we have a table that translates these codes into actual airline names\nairlines_input_rdd = sc.textFile(\"/mnt/%s/airlines.csv\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["airlines_input_rdd.take(5)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# lets remove the headers and convert out string to list\nairlines_rdd = airlines_input_rdd.filter(lambda line: 'IATA_CODE' not in line).map(lambda line: line.split(','))\nairlines_rdd.take(5)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# We can use join to translate the code into names\nposDepartureDelay_reduced_rdd.join(airlines_rdd).collect()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# we can keep just the airlines name and total departure delay from the join\nposDepartureDelay_reduced_rdd.join(airlines_rdd).map(lambda line: (line[1][1], line[1][0])).collect()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# lets store it and print it sorted \ntotalDepartureDelay = posDepartureDelay_reduced_rdd.join(airlines_rdd).map(lambda line: (line[1][1], line[1][0])).collect()\nsorted(totalDepartureDelay, key=lambda x: x[1])"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Can we say Hawaiian Airlines Inc. is your best bet in order to avoid late departure?\n#### No: Hawaiian Airlines Inc. may have smaller operations than the others, so let's get the mean!"],"metadata":{}},{"cell_type":"code","source":["# we already have the total delay per airline\nposDepartureDelay_reduced_rdd.collect()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# lets create a rdd of flight count per airline\nflight_count_rdd = airline_posDepartureDelay_rdd.map(lambda x: (x[0],1)).reduceByKey(lambda v1, v2: v1 + v2)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# we can join this two data to get total delay and total flight count per airline\nposDepartureDelay_flight_rdd = posDepartureDelay_reduced_rdd.join(flight_count_rdd)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# we can divide the values to get the mean\nposDepartureDelay_mean_rdd = posDepartureDelay_flight_rdd.map(lambda x: (x[0], x[1][0]/x[1][1]))\nsorted(posDepartureDelay_mean_rdd.collect(), key=lambda x: x[1])"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# we can also do one more join to print out the airlines full name\ndepartureDelay = posDepartureDelay_mean_rdd.join(airlines_rdd).map(lambda line: (line[1][1], line[1][0])).collect()\nsorted(departureDelay, key=lambda x: x[1])"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["#### lets compare with with our previous result of total departure delay, what do you see?"],"metadata":{}},{"cell_type":"code","source":["sorted(totalDepartureDelay, key=lambda x: x[1])"],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"Spark Lab 4 Public","notebookId":3097811348877471},"nbformat":4,"nbformat_minor":0}
