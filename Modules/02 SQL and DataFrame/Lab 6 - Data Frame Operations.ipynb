{"cells":[{"cell_type":"code","source":["# Similar to SparkContext, for SparkSQL you need a SparkSession\nfrom pyspark.sql import SparkSession\n# Also all the functions (select, where, groupby) needs to be imported\nfrom pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Get spark session\nspark = SparkSession.builder.getOrCreate()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# read data into dataframe\nratings_df = spark.read.csv(\"/FileStore/tables/movielens/ratings.csv\", header=True)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["### DataFrames Operations\n\nIn this part you will learn how to programmatically use the SQL capabilities of DataFrame. For the full list of documentation: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql"],"metadata":{}},{"cell_type":"code","source":["# You can use the select method to grab specific columns\ndisplay(ratings_df.select(['movieId','rating']))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# see how ratings are in string\nratings_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# You can change the Data type of any column by casting them to your desired data type\n# First you have to import that data type from pyspark.sql.types\nfrom pyspark.sql.types import IntegerType\n# Then you can use withColumn() to apply / cast each row of the column (Notice how the square bracket annotation is used)\nratings_df = ratings_df.withColumn(\"rating\", ratings_df['rating'].cast(IntegerType()))\n# take a look at the schema now\nratings_df.select(['movieId','rating']).printSchema()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# You can use the filter() here to filter on a condition (just like we did with RDD!)\n# For example we can check if there are any missing ratings \nratings_df.filter(ratings_df.rating.isNull()).count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# similar to filter you can also use where (from SQL syntax)\nratings_df.where(ratings_df.rating.isNull()).count()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Group By\nThe GROUP BY statement is used with **aggregate functions (COUNT, MAX, MIN, SUM, AVG)** to group the result-set by one or more columns."],"metadata":{}},{"cell_type":"code","source":["# For instance, we can group by the movieId over rating and aggregate over the average value and total reviews (very easily)\ndisplay(ratings_df.groupBy('movieId').agg(avg('rating').alias('avg_rating'), count('rating').alias('reviews')))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# We can also see the top 10 rated movies if they have been reviewed at least 50 times or more\nratings_sum_df = ratings_df.groupBy('movieId').agg(avg('rating').alias('avg_rating'), count('rating').alias('reviews'))\ndisplay(ratings_sum_df.filter(ratings_sum_df.reviews > 50).sort('avg_rating', ascending=False).limit(10))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### User Defined Functions (UDF)\nSimilar to custom functions for Map, you can write user defined function to transform one or more columns. \nMore about UDF on https://docs.databricks.com/spark/latest/spark-sql/udf-in-python.html"],"metadata":{}},{"cell_type":"code","source":["# Using UDF is a three step process. Before anything you will need to import the udf library\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# If you can express your user defined function as lambda then you can register the UDF and define it in one line like below\n# for example this UDF will tell me if I should watch a movie or not based on its average rating\nwatchable_udf = udf(lambda avg_rating: 'yes' if avg_rating > 3.5 else 'no', StringType())"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Otherwise you can first write your function\n# as you can see here we have more flexibility\n# I will write the function to also incorporate the total number of reviews\ndef watchable_udf(avg_rating, reviews):\n  if avg_rating > 3.5 and reviews > 50:\n    return 'yes'\n  elif avg_rating > 3.5 and reviews < 50:\n    return 'maybe'\n  else:\n    return 'no'\n# and then register it as an UDF with the return type declared\nwatchable_udf = udf(watchable_udf, StringType())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Now you can use withColumn to apply the UDF over every row and create a new column 'watchable'\nratings_sum_df = ratings_sum_df.withColumn('watchable', watchable_udf(ratings_sum_df.avg_rating,ratings_sum_df.reviews))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["display(ratings_sum_df)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Joins\nA JOIN clause is used to combine rows from two or more tables, based on a related column between them. Here are the a few basic types of joins explained:\n\n* (INNER) JOIN: Returns records that have matching values in both tables\n* LEFT (OUTER) JOIN: Return all records from the left table, and the matched records from the right table\n* RIGHT (OUTER) JOIN: Return all records from the right table, and the matched records from the left table\n* FULL (OUTER) JOIN: Return all records when there is a match in either left or right table\n\nSpark Supports more than just basic joins however. With the latest spark you get: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti joins! Take a look in  https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#join for more details."],"metadata":{}},{"cell_type":"code","source":["# lets use the movies csv file to make sense of the movies in our previous results\nmovies_df = spark.read.csv(\"/FileStore/tables/movielens/movies.csv\", header=True)\ndisplay(movies_df)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# we do an inner join to get more information about each movies\nmovie_ratings_sum_df = ratings_sum_df.join(movies_df, ratings_sum_df.movieId == movies_df.movieId)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# lets display a few results\ndisplay(movie_ratings_sum_df.select(['title','avg_rating','reviews','watchable']))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Challenge: Can you create a table of the highest rated movie per category?"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"Spark Lab 6","notebookId":1713846382052925},"nbformat":4,"nbformat_minor":0}
