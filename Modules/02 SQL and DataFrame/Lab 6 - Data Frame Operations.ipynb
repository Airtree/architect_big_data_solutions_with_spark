{"cells":[{"cell_type":"code","source":["# Put the correct credentials here and mount S3 bucket\nACCESS_KEY = \"\"\nSECRET_KEY = \"\"\nAWS_BUCKET_NAME = \"\"\nMOUNT_NAME = \"\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n\ntry: \n  dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\nexcept:\n  pass"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Similar to SparkContext, for SparkSQL you need a SparkSession\nfrom pyspark.sql import SparkSession\n# Also all the functions (select, where, groupby) needs to be imported\nfrom pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Get spark session\nspark = SparkSession.builder.getOrCreate()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# read data into dataframe\nflights_df = spark.read.csv(\"/mnt/%s/flights.csv\" % MOUNT_NAME, header=True)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### DataFrames Operations\n\nIn this part you will learn how to programmatically use the SQL capabilities of DataFrame. For the full list of documentation: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql"],"metadata":{}},{"cell_type":"code","source":["# You can use the select method to grab specific columns\ndisplay(flights_df.select(['AIRLINE','DEPARTURE_DELAY']))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# You can change the Data type of any column by casting them to your desired data type\n# First you have to import that data type from pyspark.sql.types\nfrom pyspark.sql.types import IntegerType\n# Then you can use withColumn() to apply / cast each row of the column (Notice how the square bracket annotation is used)\nflights_df = flights_df.withColumn(\"DEPARTURE_DELAY\", flights_df['DEPARTURE_DELAY'].cast(IntegerType()))\n# take a look at the schema now\nflights_df.select(['AIRLINE','DEPARTURE_DELAY']).printSchema()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# You can use the filter() here to filter on a condition (just like we did with RDD!)\n# As you can see there is a lot of null values of departure. For empty strings '' values couldn't be converted to Integer and hence they were turned to Null values\nflights_df.filter(flights_df.DEPARTURE_DELAY.isNull()).count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# that is around 1.4 % of our total data!\n(float(flights_df.filter(flights_df.DEPARTURE_DELAY.isNull()).count()) / flights_df.count()) * 100"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# get the mean of departure delay\nflights_df.filter(flights_df.DEPARTURE_DELAY.isNotNull()).agg(avg(flights_df.DEPARTURE_DELAY)).collect()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# we can specify that we want to fillna for only one column by using the subset argument in the following way\nflights_df = flights_df.fillna(9.37, subset=['DEPARTURE_DELAY'])"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# now if you take a look at the count of null values you can be convinced our previous steps worked!\nflights_df.filter(flights_df.DEPARTURE_DELAY.isNull()).count()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### User Defined Functions (UDF)\nSimilar to custom functions for Map, you can write user defined function to transform one or more columns. \nMore about UDF on https://docs.databricks.com/spark/latest/spark-sql/udf-in-python.html"],"metadata":{}},{"cell_type":"code","source":["# Using UDF is a three step process. Before anything you will need to import the udf library\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# If you can express your user defined function as lambda then you can register the UDF and define it in one line like below\nformat_delay_udf = udf(lambda delay: delay if delay > 0 else 0, IntegerType())"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Otherwise you can first write your function\ndef format_delay(delay):\n  return delay if delay > 0 else 0\n# and then register it as an UDF with the return type declared\nformat_delay_udf = udf(format_delay, IntegerType())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Now you can use withColumn to apply the UDF over every row for DEPARTURE_DELAY column\nflights_df = flights_df.withColumn('DEPARTURE_DELAY', format_delay_udf(flights_df['DEPARTURE_DELAY']))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Group By\nThe GROUP BY statement is used with **aggregate functions (COUNT, MAX, MIN, SUM, AVG)** to group the result-set by one or more columns."],"metadata":{}},{"cell_type":"code","source":["# For instance, we can group by the DEPARTURE DELAY over AIRLINE and aggregate over the average value\ndisplay(flights_df.groupBy('AIRLINE').agg(avg('DEPARTURE_DELAY').alias('AVG_DEPARTURE_DELAY')))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# we can also display how many times each airline flew in 2015 by using the count aggregate function\ndisplay(flights_df.groupBy('AIRLINE').agg(count('AIRLINE').alias('FLIGHT_COUNT')))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# We can also Group By over different departure airports, lets limit to top 20!\ndep_delay_by_airport_df = flights_df.groupBy('ORIGIN_AIRPORT').agg(avg('DEPARTURE_DELAY').alias('AVG_DEPARTURE_DELAY'))\ndisplay(dep_delay_by_airport_df.sort('AVG_DEPARTURE_DELAY', ascending=False).limit(10))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Joins\nA JOIN clause is used to combine rows from two or more tables, based on a related column between them. Here are the a few basic types of joins explained:\n\n* (INNER) JOIN: Returns records that have matching values in both tables\n* LEFT (OUTER) JOIN: Return all records from the left table, and the matched records from the right table\n* RIGHT (OUTER) JOIN: Return all records from the right table, and the matched records from the left table\n* FULL (OUTER) JOIN: Return all records when there is a match in either left or right table\n\nSpark Supports more than just basic joins however. With the latest spark you get: inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and left_anti joins! Take a look in  https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#join for more details."],"metadata":{}},{"cell_type":"code","source":["# lets use the airports csv file to make sense of the airports in our previous results\nairports_df = spark.read.csv(\"/mnt/%s/airports.csv\" % MOUNT_NAME, header=True)\ndisplay(airports_df)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# we do an inner join to get more information about each airport\ndep_delay_by_airport_df = dep_delay_by_airport_df.join(airports_df, dep_delay_by_airport_df['ORIGIN_AIRPORT'] == airports_df['IATA_CODE'])"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# you can use sort() and then limit() to get the top n number of results back\ndisplay(dep_delay_by_airport_df.sort('AVG_DEPARTURE_DELAY', ascending=False).limit(10))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# unfortunately we have a few extra entries for state in our dataset\ndep_delay_by_airport_df.select('STATE').distinct().count()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Visualization in DataBricks\nHere we take advantage of the built in plots in Databrick's notebook to visualize average delay per state. As we have some state entry in our list, I will load a csv of States in USA and do an inner join to get rid of invalid entries."],"metadata":{}},{"cell_type":"code","source":["# load the usa states dataset\nstate_codes_df = spark.read.csv(\"/mnt/%s/usa_state_codes.csv\" % MOUNT_NAME, header=True)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# we use the display and then plot option to visualize the average delay per state\ndisplay(dep_delay_by_airport_df.join(state_codes_df, dep_delay_by_airport_df['STATE']==state_codes_df['Code'],how='inner'))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["x= dep_delay_by_airport_df.join(state_codes_df, dep_delay_by_airport_df['STATE']==state_codes_df['Code'],how='inner')"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["### Challenge: Can you make the same geo-plot for arrival delay?"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":32}],"metadata":{"name":"Spark Lab 6","notebookId":116855200887280},"nbformat":4,"nbformat_minor":0}
