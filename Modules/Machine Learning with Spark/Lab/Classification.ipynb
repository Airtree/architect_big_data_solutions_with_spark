{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libs\n",
    "\n",
    "# import necessary libs\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "# general spark modules\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "# spark ml modules \n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# classification \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data as dataframe\n",
    "train_df = spark.read.csv('/projects/hue_kdd/derived/shared/talking_data/train.csv', header=True)\n",
    "test_df  = spark.read.csv('/projects/hue_kdd/derived/shared/talking_data/test.csv',  header=True)\n",
    "\n",
    "# Drop not used columns \n",
    "# train_df = train_df.drop('attributed_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train and validation split\n",
    "train_df = train_df.withColumn('day',   substring(train_df.click_time, 9, 2))\\\n",
    "                    .withColumn('hour', substring(train_df.click_time, 12, 2))\\\n",
    "                    .withColumn('min',  substring(train_df.click_time, 15, 2))\\\n",
    "                    .withColumn('sec',  substring(train_df.click_time, 18, 2))\\\n",
    "            \n",
    "            \n",
    "test_df  = test_df.withColumn('hour', substring(test_df.click_time, 12, 2))\\\n",
    "                  .withColumn('min',  substring(test_df.click_time, 15, 2))\\\n",
    "                  .withColumn('sec',  substring(test_df.click_time, 18, 2))\\\n",
    "        \n",
    "# split data to train and validation properly\n",
    "# first three days for training, 1 day for validation for same hours as test\n",
    "tr_df  = train_df.where((train_df.day != '09'))\n",
    "val_df = train_df.where((train_df.day == '09') & (train_df.hour.isin(['15', '11', '09', '05', \n",
    "                                                                      '06', '10', '04', '13', '14'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131886953\n",
      "30840433\n",
      "18790469\n"
     ]
    }
   ],
   "source": [
    "# print number of rows in each dataset\n",
    "print tr_df.count()\n",
    "print val_df.count()\n",
    "print test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_df = tr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## session time\n",
    "## downsampling\n",
    "## add MLP\n",
    "## add GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_df(var_1, var_2, main_df, train_df, val_df, test_df):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    prefix = var_1 + '_' + var_2\n",
    "    \n",
    "    # first group by the two variables and pivot on clicked vs not clicked for their count\n",
    "    feature_count_df = main_df.groupby([var_1, var_2])\\\n",
    "                               .pivot('is_attributed').count()\\\n",
    "                               .alias(prefix+'_count').fillna(0)\\\n",
    "    \n",
    "    # rename the columns of clicked vs not clicked\n",
    "    feature_count_df = feature_count_df.withColumnRenamed('1', prefix + '_attributed')\\\n",
    "                                       .withColumnRenamed('0', prefix + '_not_attributed')\n",
    "    \n",
    "    # calculate ratio of click vs not clicked\n",
    "    feature_count_df = feature_count_df.withColumn(prefix+'_click_ratio', col(prefix+'_attributed') \\\n",
    "                                        / (col(prefix+'_attributed')+col(prefix+'_not_attributed')))\n",
    "    \n",
    "    # create a join column\n",
    "    feature_count_df = feature_count_df.withColumn('separator', lit('_')) \\\n",
    "                                       .withColumn(prefix+'_join_id', \n",
    "                                                   concat(var_1, 'separator', var_2))\n",
    "    feature_count_df = feature_count_df.drop('separator')\n",
    "    \n",
    "    # drop columns\n",
    "    feature_count_df = feature_count_df.select(prefix+'_join_id', \n",
    "                                               prefix+'_attributed', \n",
    "                                               prefix+'_not_attributed',\n",
    "                                               prefix+'_click_ratio')\n",
    "    \n",
    "    # join with main df\n",
    "    train_df_featured = train_df.withColumn('separator', lit('_'))\\\n",
    "                                .withColumn(prefix+'_join_id', concat(var_1, 'separator', var_2))\\\n",
    "                                .join(feature_count_df, on = prefix+'_join_id', how='leftouter')\n",
    "            \n",
    "    val_df_featured   = val_df.withColumn('separator', lit('_'))\\\n",
    "                                .withColumn(prefix+'_join_id', concat(var_1, 'separator', var_2))\\\n",
    "                                .join(feature_count_df, on = prefix+'_join_id', how='leftouter')\n",
    "\n",
    "    test_df_featured  = test_df.withColumn('separator', lit('_'))\\\n",
    "                               .withColumn(prefix+'_join_id', concat(var_1, 'separator', var_2))\\\n",
    "                               .join(feature_count_df, on = prefix+'_join_id', how='leftouter')\n",
    "            \n",
    "    # drop join column\n",
    "    train_df_featured = train_df_featured.drop(prefix+'_join_id')\n",
    "    train_df_featured = train_df_featured.drop('separator')\n",
    "    \n",
    "    val_df_featured   = val_df_featured.drop(prefix+'_join_id')\n",
    "    val_df_featured   = val_df_featured.drop('separator')\n",
    "    \n",
    "    test_df_featured  = test_df_featured.drop(prefix+'_join_id')\n",
    "    test_df_featured  = test_df_featured.drop('separator')\n",
    "    \n",
    "    # fill missing values with 0\n",
    "    train_df_featured   = train_df_featured.fillna(0)\n",
    "    val_df_featured     = val_df_featured.fillna(0)\n",
    "    test_df_featured    = test_df_featured.fillna(0)\n",
    "    \n",
    "    print \"--- %s seconds ---\" % (time.time() - start_time)\n",
    "    print \n",
    "    \n",
    "    return train_df_featured, val_df_featured, test_df_featured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_features_per_ip(main_df, train_df, val_df, test_df):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    features_df = main_df.groupBy(col('ip')).agg(collect_set('os').alias('os_unique'),\n",
    "                                collect_set('app').alias('app_unique'),\n",
    "                                collect_set('channel').alias('channel_unique'),\n",
    "                                collect_set('device').alias('device_unique'),\n",
    "                                collect_list('click_time').alias('clicks'),\n",
    "                                collect_list('attributed_time').alias('download_clicks'))\n",
    "    \n",
    "    features_df = features_df.withColumn('os_unique_count',       size('os_unique'))\\\n",
    "                             .withColumn('app_unique_count',      size('app_unique'))\\\n",
    "                             .withColumn('channel_unique_count',  size('channel_unique'))\\\n",
    "                             .withColumn('device_unique_count',   size('device_unique'))\\\n",
    "                             .withColumn('clicks_count',          size('clicks'))\\\n",
    "                             .withColumn('download_clicks',       size('download_clicks'))\\\n",
    "                             .withColumn('download_rate',        col('download_clicks')/col('clicks_count'))\n",
    "     \n",
    "    features_df = features_df.select('ip', 'os_unique_count', 'app_unique_count', 'channel_unique_count',\n",
    "                                     'device_unique_count', 'clicks_count', 'download_clicks', 'download_rate') \n",
    "                            \n",
    "    # join with main df\n",
    "    train_df_featured = train_df.join(features_df, on = 'ip', how='leftouter')\n",
    "    val_df_featured   = val_df.join(features_df,   on = 'ip', how='leftouter')\n",
    "    test_df_featured  = test_df.join(features_df,  on = 'ip', how='leftouter')\n",
    "    \n",
    "    # fill missing values with 0\n",
    "    train_df_featured   = train_df_featured.fillna(0)\n",
    "    val_df_featured     = val_df_featured.fillna(0)\n",
    "    test_df_featured    = test_df_featured.fillna(0)\n",
    "    \n",
    "    print \"--- %s seconds ---\" % (time.time() - start_time)\n",
    "    print \n",
    "    \n",
    "    return train_df_featured, val_df_featured, test_df_featured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.189445018768 seconds ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get unique features for ip\n",
    "tr_df, val_df, test_df = get_unique_features_per_ip(main_df, tr_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 8.97741413116 seconds ---\n",
      "\n",
      "--- 11.644203186 seconds ---\n",
      "\n",
      "--- 11.7373659611 seconds ---\n",
      "\n",
      "--- 6.65888690948 seconds ---\n",
      "\n",
      "--- 10.8854689598 seconds ---\n",
      "\n",
      "--- 10.0641140938 seconds ---\n",
      "\n",
      "--- 7.20223808289 seconds ---\n",
      "\n",
      "--- 7.70186305046 seconds ---\n",
      "\n",
      "--- 7.24928689003 seconds ---\n",
      "\n",
      "--- 10.2056500912 seconds ---\n",
      "\n",
      "--- 7.22589707375 seconds ---\n",
      "\n",
      "--- 10.9927330017 seconds ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# learning with counts\n",
    "\n",
    "# chanell\n",
    "var_1      = 'channel'\n",
    "var_2_list = ['app', 'device', 'os', 'ip', 'hour']\n",
    "\n",
    "for var_2 in var_2_list:\n",
    "    tr_df, val_df, test_df = get_features_df(var_1, var_2, main_df, tr_df, val_df, test_df)\n",
    "    \n",
    "# ip\n",
    "var_1      = 'ip'\n",
    "var_2_list = ['app', 'device', 'os', 'hour']\n",
    "\n",
    "for var_2 in var_2_list:\n",
    "    tr_df, val_df, test_df = get_features_df(var_1, var_2, main_df, tr_df, val_df, test_df)\n",
    "    \n",
    "# app\n",
    "var_1      = 'app'\n",
    "var_2_list = ['device', 'os', 'hour']\n",
    "\n",
    "for var_2 in var_2_list:\n",
    "    tr_df, val_df, test_df = get_features_df(var_1, var_2, main_df, tr_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_df    = tr_df.drop(  'ip',    'click_time', 'attributed_time', 'day', 'min', 'sec')\n",
    "val_df   = val_df.drop( 'ip',   'click_time', 'attributed_time', 'day', 'min', 'sec')\n",
    "test_df  = test_df.drop('ip',  'click_time', 'attributed_time', 'day', 'min', 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data for models\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns# Write \n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cast numerical columns to float\n",
    "numerical_cols = tr_df.columns[6:]\n",
    "label_col = ['is_attributed']\n",
    "tr_df   = convertColumn(tr_df,   numerical_cols + label_col, FloatType())\n",
    "val_df  = convertColumn(val_df,  numerical_cols + label_col, FloatType())\n",
    "test_df = convertColumn(test_df, numerical_cols, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill missing values with 0\n",
    "tr_df   = tr_df.fillna(0)\n",
    "val_df  = val_df.fillna(0)\n",
    "test_df = test_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stages = []\n",
    "\n",
    "numerical_cols   = tr_df.columns[6:]\n",
    "categorical_cols = ['hour'] \n",
    "#                     'app', 'device', 'os', 'channel']\n",
    "\n",
    "for categorical_col in categorical_cols:\n",
    "    string_indexer = StringIndexer(inputCol=categorical_col, outputCol=categorical_col + \"_index\")\n",
    "    stages += [string_indexer]\n",
    "\n",
    "assembler_inputs = numerical_cols + [c + \"_index\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "stages  += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(tr_df)\n",
    "train         = pipelineModel.transform(tr_df)\n",
    "val           = pipelineModel.transform(val_df)\n",
    "test          = pipelineModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.repartition(100)\n",
    "train = train.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "regParam        = [0.1, 0.5, 2.0]\n",
    "elasticNetParam = [0.0,  0.5, 1.0]\n",
    "maxIter         = [10, 50, 100]\n",
    "experiments     = list(itertools.product(regParam, elasticNetParam, maxIter))\n",
    "print len(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, experiment in enumerate(experiments):\n",
    "    regParam        = experiment[0]\n",
    "    elasticNetParam = experiment[1]\n",
    "    maxIter         = experiment[2]\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    print ind\n",
    "    print 'params: ', regParam, elasticNetParam, maxIter\n",
    "    \n",
    "    lr = LogisticRegression(labelCol=\"is_attributed\", \n",
    "                            featuresCol=\"features\", \n",
    "                            regParam=regParam,\n",
    "                            elasticNetParam=elasticNetParam,\n",
    "                            maxIter=maxIter\n",
    "                            )\n",
    "    \n",
    "    # Train model with Training Data\n",
    "    lrModel     = lr.fit(train)\n",
    "    \n",
    "    # Make predictions on validation data using the transform() method.\n",
    "    # LogisticRegression.transform() will only use the 'features' column.\n",
    "    predictions = lrModel.transform(val)\n",
    "    \n",
    "    # evaluate predictions\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol='is_attributed')\n",
    "    auc       = evaluator.evaluate(predictions)\n",
    "    print 'AUC: ', auc\n",
    "    print \"--- %s seconds ---\" % (time.time() - start_time)\n",
    "    print \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0\n",
    "params:  0.1 0.0 10\n",
    "AUC:  0.954670766323\n",
    "--- 441.857987881 seconds ---\n",
    "\n",
    "1\n",
    "params:  0.1 0.0 50\n",
    "AUC:  0.955339694913\n",
    "--- 299.4097929 seconds ---\n",
    "\n",
    "2\n",
    "params:  0.1 0.0 100\n",
    "AUC:  0.955339694913\n",
    "--- 297.301779032 seconds ---\n",
    "\n",
    "3\n",
    "params:  0.1 0.5 10\n",
    "AUC:  0.5\n",
    "--- 245.735084057 seconds ---\n",
    "\n",
    "4\n",
    "params:  0.1 0.5 50\n",
    "AUC:  0.5\n",
    "--- 233.575064182 seconds ---\n",
    "\n",
    "5\n",
    "params:  0.1 0.5 100\n",
    "AUC:  0.5\n",
    "--- 241.358119011 seconds ---\n",
    "\n",
    "6\n",
    "params:  0.1 1.0 10\n",
    "AUC:  0.5\n",
    "--- 223.717324018 seconds ---\n",
    "\n",
    "7\n",
    "params:  0.1 1.0 50\n",
    "AUC:  0.5\n",
    "--- 224.282985926 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"is_attributed\", \n",
    "                        featuresCol=\"features\", \n",
    "                        regParam=0.1,\n",
    "                        elasticNetParam=0,\n",
    "                        maxIter=100\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with Training Data\n",
    "lrModel     = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.955339694913\n",
      "--- 791.741017103 seconds ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol='is_attributed')\n",
    "auc       = evaluator.evaluate(predictions)\n",
    "print 'AUC: ', auc\n",
    "print \"--- %s seconds ---\" % (time.time() - start_time)\n",
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+----------+\n",
      "|click_id|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "|13184596|[6.56511485417077...|[0.99859331938684...|       0.0|\n",
      "|15512127|[6.60834346139872...|[0.99865275244681...|       0.0|\n",
      "|13958723|[6.44647258453791...|[0.99841640458045...|       0.0|\n",
      "|14430045|[6.54686916884697...|[0.99856745506389...|       0.0|\n",
      "+--------+--------------------+--------------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('click_id', 'rawPrediction', 'probability', 'prediction').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('click_id', 'prediction').withColumn('is_attributed', col('prediction')).coalesce(1).write.csv('mycsv2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "maxDepth = [15, 30]\n",
    "maxBins  = [60, 80]\n",
    "experiments     = list(itertools.product(maxDepth, maxBins))\n",
    "print len(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, experiment in enumerate(experiments):\n",
    "    maxDepth = experiment[0]\n",
    "    maxBins  = experiment[1]\n",
    "\n",
    "    start_time = time.time()\n",
    "    print ind\n",
    "    print 'params: ', maxDepth, maxBins\n",
    "    \n",
    "    # Create initial Decision Tree Model\n",
    "    dt = DecisionTreeClassifier(labelCol=\"is_attributed\", featuresCol=\"features\", maxDepth=maxDepth, maxBins=maxBins)\n",
    "    \n",
    "    # Train model with Training Data\n",
    "    dtModel = dt.fit(train)\n",
    "    \n",
    "    # Make predictions on validation data using the transform() method.\n",
    "    # LogisticRegression.transform() will only use the 'features' column.\n",
    "    predictions = dtModel.transform(val)\n",
    "    \n",
    "    # evaluate predictions\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol='is_attributed')\n",
    "    auc       = evaluator.evaluate(predictions)\n",
    "    print 'AUC: ', auc\n",
    "    print \"--- %s seconds ---\" % (time.time() - start_time)\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0\n",
    "params:  15 60\n",
    "AUC:  0.513338548522\n",
    "--- 269.140621185 seconds ---\n",
    "\n",
    "1\n",
    "params:  15 80\n",
    "AUC:  0.515574417785\n",
    "--- 227.470299959 seconds ---\n",
    "\n",
    "2\n",
    "params:  30 60\n",
    "AUC:  0.515309944869\n",
    "--- 397.263484001 seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "numTrees         = [5, 10, 50, 100]\n",
    "subsamplingRate  = [0.8]\n",
    "maxDepth         = [10, 15]\n",
    "experiments      = list(itertools.product(numTrees, maxDepth, subsamplingRate))\n",
    "print len(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, experiment in enumerate(experiments):\n",
    "    numTrees = experiment[0]\n",
    "    maxDepth = experiment[1]\n",
    "    subsamplingRate = experiment[2]\n",
    "\n",
    "    start_time = time.time()\n",
    "    print ind\n",
    "    print 'params: ', numTrees, maxDepth, subsamplingRate\n",
    "    \n",
    "    # Create an initial RandomForest model.\n",
    "    rf = RandomForestClassifier(labelCol=\"is_attributed\", featuresCol=\"features\", \n",
    "                                numTrees=numTrees, \n",
    "                                maxDepth=maxDepth,\n",
    "                                subsamplingRate=subsamplingRate)\n",
    "    \n",
    "    # Train model with Training Data\n",
    "    rfModel = rf.fit(train)\n",
    "    \n",
    "    # Make predictions on validation data using the transform() method.\n",
    "    # LogisticRegression.transform() will only use the 'features' column.\n",
    "    predictions = rfModel.transform(val)\n",
    "    \n",
    "    # evaluate predictions\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol='is_attributed')\n",
    "    auc       = evaluator.evaluate(predictions)\n",
    "    print 'AUC: ', auc\n",
    "    print \"--- %s seconds ---\" % (time.time() - start_time)\n",
    "    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0\n",
    "params:  5 10 0.8\n",
    "AUC:  0.524119692289\n",
    "--- 278.628679991 seconds ---\n",
    "\n",
    "1\n",
    "params:  5 15 0.8\n",
    "AUC:  0.513582253066\n",
    "--- 434.034000158 seconds ---\n",
    "\n",
    "2\n",
    "params:  10 10 0.8\n",
    "AUC:  0.509167695992\n",
    "--- 450.640409946 seconds ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
