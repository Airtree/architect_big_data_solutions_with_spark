{"cells":[{"cell_type":"code","source":["# load the dataset into an RDD to get started\ninput_rdd = sc.textFile(\"/FileStore/tables/movielens/movies.csv\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":["### Transformations\nTransformations are operations that will not be completed at the time you write and execute the code in a cell - they will only get executed once you have called an action. An example of a transformation might be to convert an integer into a float or to filter a set of values. In this section we will discuss the basic transformations that can be applied on top of RDD.\n##### map\nThe map method is a higher-order method that takes a function as input and applies it to each element in\nthe source RDD to create a new RDD. The input function to map must take a single input parameter and\nreturn a value."],"metadata":{}},{"cell_type":"code","source":["# just to show you the first line of the RDD\ninput_rdd.first()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# notice how the whole line is one single string\n# using the map function, you can transfer the previous RDD to have a list of values instead\ninput_list = input_rdd.map(lambda line: line.split(','))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# They are a list object now, instead of pure string\n# First is an action. ONLY AT THIS POINT SPARK WILL START PROCESSING\n# map is a transformation, which are lazily evaluated. When we executed the map function spark didn't do anything until an action was called upon.\ninput_list.first()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["##### filter\nThe filter method is a higher-order method that takes a Boolean function as input and applies it to each\nelement in the source RDD to create a new RDD. A Boolean function takes an input and returns true or\nfalse. The filter method returns a new RDD formed by selecting only those elements for which the input\nBoolean function returned true. Thus, the new RDD contains a subset of the elements in the original RDD."],"metadata":{}},{"cell_type":"code","source":["# the original input RDD has header\ninput_rdd.take(4)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# for processing the data I can get rid of the header with a filter operation\nmovie_info_rdd = input_rdd.filter(lambda line: 'movieId' not in line)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# we got rid of the header\n# again, only at this point spark processes the data\nmovie_info_rdd.first()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# lets keep a list rdd for further examples\nmovie_info_list_rdd = movie_info_rdd.map(lambda x: x.split(','))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["##### flatMap\nThe flatMap method is a higher-order method that takes an input function, which returns a sequence for\neach input element passed to it. The flatMap method returns a new RDD formed by flattening this collection\nof sequence. The concept and usefulness of a flatMap can be easily explained with the following example:"],"metadata":{}},{"cell_type":"code","source":["# notice the last field of the movie info, you have multiple categories associated with a single movie\nmovie_info_list_rdd.first()[-1]"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# if we want to do a count each categories appear, we can use flatmap to easily get our answer\n# using a flatmap on top of the movie category element causes each entry within the list (categories) to be a single entry/line/tuple in our RDD\nmovie_cat_rdd = movie_info_list_rdd.flatMap(lambda x: x[-1].split('|'))\nmovie_cat_rdd.take(10)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# now we can easily get our category count by using the count by value action (which does exactly what the name suggests).\ncat_count = movie_cat_rdd.countByValue()\n# we can print out the result in a sorted way using python\nsorted(cat_count.items(), key=lambda k_v: k_v[1], reverse=True)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["##### Writing Custom Functions\nYou can write custom functions to process each line within RDD, as illustrated below.\n\nTake the following problem: We want to know what is the oldest movie we have in our dataset. In the movie name the year is given in paranthesis, i.e. Toy Story (1995). We need to extract this year from every name and then convert it to integer. Then we can use the action 'min' to find the minimum value in our data. However, to extract this year, we need to do some special processing. This is where our custom function will come in handy."],"metadata":{}},{"cell_type":"code","source":["# we will extract the year with this function. if there is problem with our data we will just return a None value\nimport re\ndef get_year(name):\n    year = None\n    try:\n      pattern = re.compile(r\"\\((\\d{4})\\)\")\n      year = int(pattern.findall(name)[0])\n    except ValueError:\n      pass\n    except IndexError:\n      pass\n    \n    return year"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# we can use the map operation to apply our custom function to every name in our rdd\nmovie_year_rdd = movie_info_list_rdd.map(lambda x: get_year(x[1]))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# we can use the min action to get the oldest movie year. however as you see there was some issue with the data or parsing and we are getting 6 back as our result\nmovie_year_rdd.filter(lambda x: x is not None).min()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# so instead of trying to investigate what happened we will simply apply a filter and only consider value above 1000 to get our oldest movie year\nmovie_year_rdd.filter(lambda x: x is not None).filter(lambda x: x  > 1000).min()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["movie_year_rdd.filter(lambda x: x is not None).max()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["#### Read The Docs\nOf course these are just the basic, there are many more transformation operations you can do with your RDD. All of them can be found in the official documentaiton: https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\nHere are a few more just to give you some idea:"],"metadata":{}},{"cell_type":"markdown","source":["##### union\nThe union method takes an RDD as input and returns a new RDD that contains the union of the elements in the source RDD and the RDD passed to it as an input.\n\n```linesFile1 = sc.textFile(\"...\")\nlinesFile2 = sc.textFile(\"...\")\nlinesFromBothFiles = linesFile1.union(linesFile2)```"],"metadata":{}},{"cell_type":"code","source":["mammals = sc.parallelize([\"Lion\", \"Dolphin\", \"Whale\"])\naquatics = sc.parallelize([\"Shark\", \"Dolphin\", \"Whale\"])\nzoo = mammals.union(aquatics)\nzoo.collect()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["##### intersection\nThe intersection method takes an RDD as input and returns a new RDD that contains the intersection of\nthe elements in the source RDD and the RDD passed to it as an input.\n\n```val linesFile1 = sc.textFile(\"...\")\nval linesFile2 = sc.textFile(\"...\")\nval linesPresentInBothFiles = linesFile1.intersection(linesFile2)```"],"metadata":{}},{"cell_type":"code","source":["mammals = sc.parallelize([\"Lion\", \"Dolphin\", \"Whale\"])\naquatics = sc.parallelize([\"Shark\", \"Dolphin\", \"Whale\"])\naquaticMammals = mammals.intersection(aquatics)\naquaticMammals.collect()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["##### subtract\nThe subtract method takes an RDD as input and returns a new RDD that contains elements in the source\nRDD but not in the input RDD.\n```linesFile1 = sc.textFile(\"...\")\nlinesFile2 = sc.textFile(\"...\")\nlinesInFile1Only = linesFile1.subtract(linesFile2)```"],"metadata":{}},{"cell_type":"code","source":["mammals = sc.parallelize([\"Lion\", \"Dolphin\", \"Whale\"])\naquatics =sc.parallelize([])\nfishes = aquatics.subtract(mammals)\nfishes.collect()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["##### distinct\nThe distinct method of an RDD returns a new RDD containing the distinct elements in the source RDD"],"metadata":{}},{"cell_type":"code","source":["sc.parallelize([\"Lion\", \"Dolphin\", \"Whale\",\"Shark\", \"Dolphin\", \"Whale\"]).distinct().collect()"],"metadata":{},"outputs":[],"execution_count":29}],"metadata":{"name":"Spark Lab 2 Public","notebookId":2816521733007191},"nbformat":4,"nbformat_minor":0}
