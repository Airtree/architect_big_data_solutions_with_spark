{"cells":[{"cell_type":"markdown","source":["### Introducing Apache Spark\n\nSpark started in 2009 as a research project in the UC Berkeley RAD Lab, which later became the AMPLab. The researchers in the lab had previously been working on Hadoop MapReduce and observed that MR was inefficient for iterative and interactive computing jobs. Thus, from the beginning, Spark was designed to be fast for interactive queries and iterative algorithms, bringing in ideas such as support for in-memory storage and efficient fault recovery.\n\nIn 2011, the AMPLab started to develop higher-level components on Spark such as Shark and Spark Streaming. These components are sometimes referred to as the Berkeley Data Analytics Stack (BDAS). Spark was first open sourced in March 2010 and transferred to the Apache Software Foundation in June 2013.\n\nIn February 2014, it became a top-level project at the Apache Software Foundation. Spark has since become one of the largest open source communities in Big Data. Now, over 250 contributors in over 50 organizations are contributing to Spark development. The user base has increased tremendously from small companies to Fortune 500 companies."],"metadata":{}},{"cell_type":"markdown","source":["### What is Apache Spark?\nLet's understand what Apache Spark is and what makes it a force to reckon with in Big Data analytics:\n* Apache Spark is a fast enterprise-grade large-scale data processing engine, which is interoperable with Apache Hadoop.\n* It is written in Scala, which is both an object-oriented and functional programming language that runs in a JVM.\n* Spark enables applications to distribute data reliably in-memory during processing. This is the key to Spark's performance as it allows applications to avoid expensive disk access and performs computations at memory speeds.\n* It is suitable for iterative algorithms by having every iteration access data through memory.\n* Spark programs perform 100 times faster than MR in-memory or 10 times faster on disk (http://spark.apache.org/).\n* It provides native support for Java, Scala, Python, and R languages with interactive shells for Scala, Python, and R. Applications can be developed easily, and often 2 to 10 times less code is needed.\n* Spark powers a stack of libraries, including Spark SQL and DataFrames for interactive analytics, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for real-time analytics. You can combine these features seamlessly in the same application.\n* Spark runs on Hadoop, Mesos, standalone cluster managers, on-premise hardware, or in the cloud"],"metadata":{}},{"cell_type":"markdown","source":["### What Apache Spark is not\nHadoop provides HDFS for storage and MR for compute. However, Spark does not provide any specific storage medium. Spark is mainly a compute engine, but you can store data in-memory or on Tachyon to process it.\n\nSpark has the ability to create distributed datasets from any file stored in the HDFS or other storage systems supported by Hadoop APIs (including your local filesystem, Amazon S3, Cassandra, Hive, HBase, Elasticsearch, and others).\n\nIt's important to note that Spark is not Hadoop and does not require Hadoop to run it. It simply has support for storage systems implementing Hadoop APIs. Spark supports text files, sequence files, Avro, Parquet, and any other Hadoop InputFormat. \n\n*Source: Big Data Analytics - Venkat Ankam*"],"metadata":{}},{"cell_type":"markdown","source":["### Load SparkContext\n\nSpark context sets up internal services and establishes a connection to a Spark execution environment.\n\nOnce a SparkContext is created you can use it to create RDDs, accumulators and broadcast variables, access Spark services and run jobs (until SparkContext is stopped).\n\n*Source: Mastering Apache Spark 2 (https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details)*"],"metadata":{}},{"cell_type":"code","source":["from pyspark import SparkContext, SparkConf"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["if not 'sc' in globals(): # This 'trick' makes sure the SparkContext sc is initialized exactly once\n    conf = SparkConf().setMaster('local[*]')\n    sc = SparkContext(conf=conf)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# print out spark version\nsc.version"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### Resilient Distributed Dataset\nRDDs are a fundamental unit of data in Spark and Spark programming revolves around creating and performing operations on RDDs. They are immutable collections partitioned across clusters that can be rebuilt (re-computed) if a partition is lost. They are created by transforming data in a stable storage using data flow operators (map, filter, group-by) and can be cached in memory across parallel operations:\n\n* Resilient: If data in memory is lost, it can be recreated (or recomputed)\n* Distributed: Distributed across clusters\n* Dataset: Initial data can come from a file or created programmatically\n\nThere are a couple of ways to create an RDD: parallelize, or read from a file:"],"metadata":{}},{"cell_type":"code","source":["# First we will fill in our credentials and connect to the s3 bucket where the data is stored\n# Replace with your values\n# NOTE: Set the access to this notebook appropriately to protect the security of your keys.\n# Or you can delete this cell after you run the mount command below once successfully.\n\nACCESS_KEY = \"none\"\nSECRET_KEY = \"none\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"none\"\nMOUNT_NAME = \"none\""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# only execute this line once\ntry: \n  dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\nexcept:\n  pass"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### We just loaded the data of 2015 Flight Delays and Cancellations (https://www.kaggle.com/usdot/flight-delays)\n**Context**: The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time, delayed, canceled, and diverted flights is published in DOT's monthly Air Travel Consumer Report and in this dataset of 2015 flight delays and cancellations.\n\n**Acknowledgements**: The flight delay and cancellation data was collected and published by the DOT's Bureau of Transportation Statistics.\n\nReleased Under CC0: Public Domain License\n\nTo understand the data please visit: https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time"],"metadata":{}},{"cell_type":"code","source":["# For now we will just read data from a text file\ninput_rdd = sc.textFile(\"/mnt/%s/flights.csv\" % MOUNT_NAME)\n# show the first 10 instances of the data\ninput_rdd.take(10)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# count the number of rows\ninput_rdd.count()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["header = input_rdd.take(1)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["for i in header.:\n  print(i)\n  print('\\n')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### There are two types of RDD operations - transformations and actions.\n**Transformations** define new RDDs based on the current RDD. **Actions** return values\nfrom RDDs."],"metadata":{}}],"metadata":{"name":"Spark Lab 1 Public","notebookId":1304373198374454},"nbformat":4,"nbformat_minor":0}
